{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M5 XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having the data to build our XGBoost model, let's start building a baseline and predict the validation data (in our case is test data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare for the transform step and improve machine learning accuracy since algorithms tend to have a hard time dealing with high cardinality columns, we define target encoding with smoothing function. The `min_samples_leaf` define a threshold where prior and target mean (for a given category value) have the same weight. Below the threshold prior becomes more important and above mean becomes more important. How weight behaves against value counts is controlled by smoothing parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "def target_encode(trn_series=None,  \n",
    "                  target=None, \n",
    "                  min_samples_leaf=100, \n",
    "                  smoothing=10,\n",
    "                  noise_level=0.01):\n",
    "    \n",
    "    assert len(trn_series) == len(target)\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    \n",
    "    # Compute target mean \n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    \n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index \n",
    "    return add_noise(ft_trn_series, noise_level)  # add_noise(ft_tst_series, noise_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a transform function to transform features to computer-readable format by combining both label encoding and target encoding. The encoding will be appropriately apply for suitable features to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    \n",
    "    nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for feature in nan_features:\n",
    "        data[feature].fillna('unknown', inplace = True)\n",
    "        \n",
    "    #we have many missing value in nan_features which is record under the same category 'unknown'\n",
    "    #It is more appropriate to use target encoding instead of label encording   \n",
    "    cat_label = ['id','event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for feature in cat_label: \n",
    "        #Encode target labels with value between 0 and n_classes-1 \n",
    "        encoder = preprocessing.LabelEncoder() \n",
    "        #Fit label encoder and return encoded labels\n",
    "        data[feature] = encoder.fit_transform(data[feature]) \n",
    "    \n",
    "    cat_target = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "    for feature in cat_target: \n",
    "        data[feature] = target_encode(data[feature], target=data.demand)\n",
    "        \n",
    "    data = reduce_mem_usage(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 2483.02 Mb (44.7% reduction)\n"
     ]
    }
   ],
   "source": [
    "data = transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create list of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_fe(data):\n",
    "    \n",
    "    # rolling demand features\n",
    "    data['lag_t28'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28))\n",
    "    data['lag_t29'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(29))\n",
    "    data['lag_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(30))\n",
    "\n",
    "    # compute rolling mean or moving average\n",
    "    data['rolling_mean_t7'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).mean())\n",
    "    data['rolling_std_t7'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).std())\n",
    "    data['rolling_mean_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).mean())\n",
    "    data['rolling_mean_t90'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(90).mean())\n",
    "    data['rolling_mean_t180'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(180).mean())\n",
    "    data['rolling_std_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).std())\n",
    "    data['rolling_skew_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).skew())\n",
    "    data['rolling_kurt_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).kurt())\n",
    "    \n",
    "    \n",
    "    # price features: using lag of 1 week (t7), 4 weeks(t28), 12 weeks(t84), 24 weeks (t168), 52 weeks (t365) \n",
    "    data['lag_price_t1'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(7)) \n",
    "    data['price_change_t1'] = (data['lag_price_t1'] - data['sell_price']) / (data['lag_price_t1']) \n",
    "    data['rolling_price_max_t365'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1).rolling(365).max()) \n",
    "    data['price_change_t365'] = (data['rolling_price_max_t365'] - data['sell_price']) / (data['rolling_price_max_t365'])\n",
    "    data['rolling_price_std_t7'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(7).std())\n",
    "    data['rolling_price_std_t30'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(30).std())\n",
    "    data.drop(['rolling_price_max_t365', 'lag_price_t1'], inplace = True, axis = 1)\n",
    "    \n",
    "    # time features\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data['week'] = data['date'].dt.week\n",
    "    data['day'] = data['date'].dt.day\n",
    "    data['dayofweek'] = data['date'].dt.dayofweek\n",
    "    data['quarter'] = data['date'].dt.quarter\n",
    "    \n",
    "    # convert 'date' data type from datetime to integer\n",
    "    data['date'] = pd.to_datetime(data['date']).dt.strftime('%Y%m%d')\n",
    "    data['date'] = [int(strdt) for strdt in data['date']]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = simple_fe(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce memory for data set with new features so we can train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 3342.33 Mb (45.6% reduction)\n"
     ]
    }
   ],
   "source": [
    "data = reduce_mem_usage(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is referenced from this notebook: https://www.kaggle.com/jestelrod/m5-eda-plotly-lstm-neural-network-vs-xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import plot_importance, plot_tree\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select all variables in data to be features except `demand` since we are training base on time splits and the `demand` feature used are rolling features (they are extracted from the past). Including this feature in features might give the actual target variable to the model but it will also cause data-leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'demand',\n",
       "       'date', 'month', 'year', 'event_name_1', 'event_type_1', 'event_name_2',\n",
       "       'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'sell_price',\n",
       "       'lag_t28', 'lag_t29', 'lag_t30', 'rolling_mean_t7', 'rolling_std_t7',\n",
       "       'rolling_mean_t30', 'rolling_mean_t90', 'rolling_mean_t180',\n",
       "       'rolling_std_t30', 'rolling_skew_t30', 'rolling_kurt_t30',\n",
       "       'price_change_t1', 'price_change_t365', 'rolling_price_std_t7',\n",
       "       'rolling_price_std_t30', 'week', 'day', 'dayofweek', 'quarter'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target and label encoder features\n",
    "features = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'date', 'month', 'year', 'event_name_1',\n",
    "            'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'lag_t28',\n",
    "            'lag_t29', 'lag_t30', 'rolling_mean_t7', 'rolling_std_t7', 'rolling_mean_t30', 'rolling_mean_t90', \n",
    "            'rolling_mean_t180', 'rolling_std_t30', 'rolling_skew_t30', 'rolling_kurt_t30', 'price_change_t1', \n",
    "            'price_change_t365', 'rolling_price_std_t7', 'rolling_price_std_t30', 'week', 'day', 'dayofweek', \n",
    "            'quarter']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train an XGBoost classifier, we split data to the training(from 2013 to 2015), validation set(from 2015 to 2016) and test set (last 28 days).\n",
    "The training and validation set are converted from Pandas DataFrames into SVMLight format. To avoid overfitting, the `early_stopping_rounds` parameter is used to stop the training process after the test area under the curve (AUC) statistic fails to increase for 20 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgb(data):\n",
    "    \n",
    "    #XGBoost uses SVMLight data structure, not Numpy arrays or Pandas DataFrames \n",
    "    #training set\n",
    "    x_train = data[data['date'] <= 20150424]\n",
    "    y_train = x_train['demand']\n",
    "    del x_train['demand']\n",
    "    # validation set\n",
    "    x_val = data[(data['date'] > 20150424) & (data['date'] < 20160522)]\n",
    "    y_val = x_val['demand']\n",
    "    del x_val['demand']\n",
    "    \n",
    "    #del data\n",
    "    gc.collect()\n",
    "    \n",
    "    dtrain = xgb.DMatrix(x_train, y_train)\n",
    "    dval = xgb.DMatrix(x_val, y_val)\n",
    "    # used to calibrate predictions to mean of y \n",
    "    #base_y = y_train.mean()\n",
    "    \n",
    "    del x_train, y_train\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'count:poisson',          \n",
    "        'booster': 'gbtree',                        # base learner will be decision tree\n",
    "        'eval_metric': 'rmse',                      \n",
    "        'eta': 0.075,                                 # learning rate\n",
    "        'subsample': 0.75,                           # use 90% of rows in each decision tree\n",
    "        'colsample_bytree': 0.9,                    # use 90% of columns in each decision tree\n",
    "        #'max_depth': 15,                            # allow decision trees to grow to depth of 15\n",
    "        #'base_score': base_y,                       # calibrate predictions to mean of y \n",
    "        'seed': 222                                 # set random seed for reproducibility\n",
    "}\n",
    "    \n",
    "    \n",
    "    watchlist = [(dtrain, 'train'), (dval, 'val')]\n",
    "    \n",
    "    # train model\n",
    "    xgb_tg_encode = xgb.train(params,                   # set tuning parameters from above                   \n",
    "                          dtrain,                   # training data\n",
    "                          1000,                     # maximum of 1000 iterations (trees)\n",
    "                          evals=watchlist,          # use watchlist for early stopping \n",
    "                          early_stopping_rounds=20,  # stop after 20 iterations (trees) without increase in AUC\n",
    "                          verbose_eval=True)        # display iteration progress\n",
    "         \n",
    "\n",
    "    return x_val, xgb_tg_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:46:34] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost_1588600955503/work/src/gbm/gbtree.cc:138: Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "[0]\ttrain-rmse:3.70773\tval-rmse:3.68110\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 20 rounds.\n",
      "[1]\ttrain-rmse:3.70328\tval-rmse:3.67610\n",
      "[2]\ttrain-rmse:3.69730\tval-rmse:3.66924\n",
      "[3]\ttrain-rmse:3.69073\tval-rmse:3.66231\n",
      "[4]\ttrain-rmse:3.68437\tval-rmse:3.65566\n",
      "[5]\ttrain-rmse:3.67614\tval-rmse:3.64671\n",
      "[6]\ttrain-rmse:3.67123\tval-rmse:3.64161\n",
      "[7]\ttrain-rmse:3.66446\tval-rmse:3.63389\n",
      "[8]\ttrain-rmse:3.65540\tval-rmse:3.62475\n",
      "[9]\ttrain-rmse:3.64782\tval-rmse:3.61666\n",
      "[10]\ttrain-rmse:3.63885\tval-rmse:3.60716\n",
      "[11]\ttrain-rmse:3.63031\tval-rmse:3.59746\n",
      "[12]\ttrain-rmse:3.62280\tval-rmse:3.58835\n",
      "[13]\ttrain-rmse:3.61389\tval-rmse:3.57890\n",
      "[14]\ttrain-rmse:3.60464\tval-rmse:3.56835\n",
      "[15]\ttrain-rmse:3.59475\tval-rmse:3.55756\n",
      "[16]\ttrain-rmse:3.58531\tval-rmse:3.54702\n",
      "[17]\ttrain-rmse:3.57536\tval-rmse:3.53663\n",
      "[18]\ttrain-rmse:3.56427\tval-rmse:3.52479\n",
      "[19]\ttrain-rmse:3.55270\tval-rmse:3.51247\n",
      "[20]\ttrain-rmse:3.54043\tval-rmse:3.49915\n",
      "[21]\ttrain-rmse:3.53067\tval-rmse:3.48849\n",
      "[22]\ttrain-rmse:3.51879\tval-rmse:3.47581\n",
      "[23]\ttrain-rmse:3.50748\tval-rmse:3.46296\n",
      "[24]\ttrain-rmse:3.49494\tval-rmse:3.44948\n",
      "[25]\ttrain-rmse:3.48248\tval-rmse:3.43629\n",
      "[26]\ttrain-rmse:3.46901\tval-rmse:3.42215\n",
      "[27]\ttrain-rmse:3.45588\tval-rmse:3.40821\n",
      "[28]\ttrain-rmse:3.44256\tval-rmse:3.39364\n",
      "[29]\ttrain-rmse:3.42880\tval-rmse:3.37904\n",
      "[30]\ttrain-rmse:3.41423\tval-rmse:3.36341\n",
      "[31]\ttrain-rmse:3.40001\tval-rmse:3.34802\n",
      "[32]\ttrain-rmse:3.38511\tval-rmse:3.33224\n",
      "[33]\ttrain-rmse:3.37009\tval-rmse:3.31604\n",
      "[34]\ttrain-rmse:3.35564\tval-rmse:3.30001\n",
      "[35]\ttrain-rmse:3.34022\tval-rmse:3.28344\n",
      "[36]\ttrain-rmse:3.32434\tval-rmse:3.26644\n",
      "[37]\ttrain-rmse:3.30897\tval-rmse:3.24961\n",
      "[38]\ttrain-rmse:3.29296\tval-rmse:3.23249\n",
      "[39]\ttrain-rmse:3.27645\tval-rmse:3.21490\n",
      "[40]\ttrain-rmse:3.25988\tval-rmse:3.19757\n",
      "[41]\ttrain-rmse:3.24326\tval-rmse:3.17978\n",
      "[42]\ttrain-rmse:3.22631\tval-rmse:3.16179\n",
      "[43]\ttrain-rmse:3.20925\tval-rmse:3.14370\n",
      "[44]\ttrain-rmse:3.19253\tval-rmse:3.12551\n",
      "[45]\ttrain-rmse:3.17518\tval-rmse:3.10711\n",
      "[46]\ttrain-rmse:3.15757\tval-rmse:3.08856\n",
      "[47]\ttrain-rmse:3.13997\tval-rmse:3.06989\n",
      "[48]\ttrain-rmse:3.12223\tval-rmse:3.05112\n",
      "[49]\ttrain-rmse:3.10445\tval-rmse:3.03252\n",
      "[50]\ttrain-rmse:3.08644\tval-rmse:3.01364\n",
      "[51]\ttrain-rmse:3.06845\tval-rmse:2.99493\n",
      "[52]\ttrain-rmse:3.05015\tval-rmse:2.97600\n",
      "[53]\ttrain-rmse:3.03207\tval-rmse:2.95708\n",
      "[54]\ttrain-rmse:3.01403\tval-rmse:2.93847\n",
      "[55]\ttrain-rmse:2.99583\tval-rmse:2.91978\n",
      "[56]\ttrain-rmse:2.97771\tval-rmse:2.90132\n",
      "[57]\ttrain-rmse:2.95959\tval-rmse:2.88270\n",
      "[58]\ttrain-rmse:2.94163\tval-rmse:2.86404\n",
      "[59]\ttrain-rmse:2.92421\tval-rmse:2.84555\n",
      "[60]\ttrain-rmse:2.90621\tval-rmse:2.82725\n",
      "[61]\ttrain-rmse:2.88831\tval-rmse:2.80928\n",
      "[62]\ttrain-rmse:2.87084\tval-rmse:2.79199\n",
      "[63]\ttrain-rmse:2.85318\tval-rmse:2.77414\n",
      "[64]\ttrain-rmse:2.83589\tval-rmse:2.75664\n",
      "[65]\ttrain-rmse:2.81927\tval-rmse:2.73944\n",
      "[66]\ttrain-rmse:2.80217\tval-rmse:2.72226\n",
      "[67]\ttrain-rmse:2.78528\tval-rmse:2.70548\n",
      "[68]\ttrain-rmse:2.76910\tval-rmse:2.68832\n",
      "[69]\ttrain-rmse:2.75253\tval-rmse:2.67206\n",
      "[70]\ttrain-rmse:2.73653\tval-rmse:2.65559\n",
      "[71]\ttrain-rmse:2.72094\tval-rmse:2.63994\n",
      "[72]\ttrain-rmse:2.70559\tval-rmse:2.62514\n",
      "[73]\ttrain-rmse:2.69047\tval-rmse:2.61044\n",
      "[74]\ttrain-rmse:2.67589\tval-rmse:2.59569\n",
      "[75]\ttrain-rmse:2.66172\tval-rmse:2.58123\n",
      "[76]\ttrain-rmse:2.64808\tval-rmse:2.56703\n",
      "[77]\ttrain-rmse:2.63483\tval-rmse:2.55314\n",
      "[78]\ttrain-rmse:2.62206\tval-rmse:2.54002\n",
      "[79]\ttrain-rmse:2.60942\tval-rmse:2.52723\n",
      "[80]\ttrain-rmse:2.59726\tval-rmse:2.51489\n",
      "[81]\ttrain-rmse:2.58553\tval-rmse:2.50313\n",
      "[82]\ttrain-rmse:2.57466\tval-rmse:2.49220\n",
      "[83]\ttrain-rmse:2.56389\tval-rmse:2.48212\n",
      "[84]\ttrain-rmse:2.55355\tval-rmse:2.47226\n",
      "[85]\ttrain-rmse:2.54395\tval-rmse:2.46272\n",
      "[86]\ttrain-rmse:2.53456\tval-rmse:2.45280\n",
      "[87]\ttrain-rmse:2.52622\tval-rmse:2.44487\n",
      "[88]\ttrain-rmse:2.51796\tval-rmse:2.43695\n",
      "[89]\ttrain-rmse:2.51020\tval-rmse:2.42919\n",
      "[90]\ttrain-rmse:2.50280\tval-rmse:2.42202\n",
      "[91]\ttrain-rmse:2.49507\tval-rmse:2.41418\n",
      "[92]\ttrain-rmse:2.48814\tval-rmse:2.40677\n",
      "[93]\ttrain-rmse:2.48173\tval-rmse:2.40050\n",
      "[94]\ttrain-rmse:2.47575\tval-rmse:2.39461\n",
      "[95]\ttrain-rmse:2.46989\tval-rmse:2.38938\n",
      "[96]\ttrain-rmse:2.46464\tval-rmse:2.38461\n",
      "[97]\ttrain-rmse:2.45938\tval-rmse:2.37955\n",
      "[98]\ttrain-rmse:2.45481\tval-rmse:2.37578\n",
      "[99]\ttrain-rmse:2.44982\tval-rmse:2.37181\n",
      "[100]\ttrain-rmse:2.44557\tval-rmse:2.36792\n",
      "[101]\ttrain-rmse:2.44154\tval-rmse:2.36465\n",
      "[102]\ttrain-rmse:2.43786\tval-rmse:2.36121\n",
      "[103]\ttrain-rmse:2.43415\tval-rmse:2.35787\n",
      "[104]\ttrain-rmse:2.42970\tval-rmse:2.35365\n",
      "[105]\ttrain-rmse:2.42614\tval-rmse:2.35033\n",
      "[106]\ttrain-rmse:2.42318\tval-rmse:2.34737\n",
      "[107]\ttrain-rmse:2.41995\tval-rmse:2.34504\n",
      "[108]\ttrain-rmse:2.41655\tval-rmse:2.34184\n",
      "[109]\ttrain-rmse:2.41340\tval-rmse:2.33959\n",
      "[110]\ttrain-rmse:2.41036\tval-rmse:2.33741\n",
      "[111]\ttrain-rmse:2.40703\tval-rmse:2.33451\n",
      "[112]\ttrain-rmse:2.40438\tval-rmse:2.33248\n",
      "[113]\ttrain-rmse:2.40153\tval-rmse:2.33043\n",
      "[114]\ttrain-rmse:2.39900\tval-rmse:2.32822\n",
      "[115]\ttrain-rmse:2.39666\tval-rmse:2.32577\n",
      "[116]\ttrain-rmse:2.39477\tval-rmse:2.32415\n",
      "[117]\ttrain-rmse:2.39312\tval-rmse:2.32272\n",
      "[118]\ttrain-rmse:2.39171\tval-rmse:2.32142\n",
      "[119]\ttrain-rmse:2.39048\tval-rmse:2.32010\n",
      "[120]\ttrain-rmse:2.38919\tval-rmse:2.31878\n",
      "[121]\ttrain-rmse:2.38730\tval-rmse:2.31720\n",
      "[122]\ttrain-rmse:2.38587\tval-rmse:2.31613\n",
      "[123]\ttrain-rmse:2.38480\tval-rmse:2.31515\n",
      "[124]\ttrain-rmse:2.38278\tval-rmse:2.31424\n",
      "[125]\ttrain-rmse:2.38155\tval-rmse:2.31322\n",
      "[126]\ttrain-rmse:2.38039\tval-rmse:2.31210\n",
      "[127]\ttrain-rmse:2.37926\tval-rmse:2.31110\n",
      "[128]\ttrain-rmse:2.37824\tval-rmse:2.30998\n",
      "[129]\ttrain-rmse:2.37696\tval-rmse:2.30904\n",
      "[130]\ttrain-rmse:2.37607\tval-rmse:2.30817\n",
      "[131]\ttrain-rmse:2.37534\tval-rmse:2.30737\n",
      "[132]\ttrain-rmse:2.37392\tval-rmse:2.30658\n",
      "[133]\ttrain-rmse:2.37279\tval-rmse:2.30586\n",
      "[134]\ttrain-rmse:2.37159\tval-rmse:2.30493\n",
      "[135]\ttrain-rmse:2.37078\tval-rmse:2.30438\n",
      "[136]\ttrain-rmse:2.37004\tval-rmse:2.30367\n",
      "[137]\ttrain-rmse:2.36902\tval-rmse:2.30310\n",
      "[138]\ttrain-rmse:2.36797\tval-rmse:2.30243\n",
      "[139]\ttrain-rmse:2.36749\tval-rmse:2.30198\n",
      "[140]\ttrain-rmse:2.36693\tval-rmse:2.30146\n",
      "[141]\ttrain-rmse:2.36606\tval-rmse:2.30086\n",
      "[142]\ttrain-rmse:2.36564\tval-rmse:2.30047\n",
      "[143]\ttrain-rmse:2.36482\tval-rmse:2.29983\n",
      "[144]\ttrain-rmse:2.36447\tval-rmse:2.29946\n",
      "[145]\ttrain-rmse:2.36344\tval-rmse:2.29831\n",
      "[146]\ttrain-rmse:2.36172\tval-rmse:2.29812\n",
      "[147]\ttrain-rmse:2.36120\tval-rmse:2.29769\n",
      "[148]\ttrain-rmse:2.36064\tval-rmse:2.29713\n",
      "[149]\ttrain-rmse:2.36004\tval-rmse:2.29651\n",
      "[150]\ttrain-rmse:2.35974\tval-rmse:2.29645\n",
      "[151]\ttrain-rmse:2.35933\tval-rmse:2.29608\n",
      "[152]\ttrain-rmse:2.35860\tval-rmse:2.29575\n",
      "[153]\ttrain-rmse:2.35815\tval-rmse:2.29535\n",
      "[154]\ttrain-rmse:2.35747\tval-rmse:2.29486\n",
      "[155]\ttrain-rmse:2.35696\tval-rmse:2.29455\n",
      "[156]\ttrain-rmse:2.35664\tval-rmse:2.29428\n",
      "[157]\ttrain-rmse:2.35508\tval-rmse:2.29419\n",
      "[158]\ttrain-rmse:2.35448\tval-rmse:2.29367\n",
      "[159]\ttrain-rmse:2.35412\tval-rmse:2.29336\n",
      "[160]\ttrain-rmse:2.35345\tval-rmse:2.29279\n",
      "[161]\ttrain-rmse:2.35313\tval-rmse:2.29254\n",
      "[162]\ttrain-rmse:2.35251\tval-rmse:2.29202\n",
      "[163]\ttrain-rmse:2.35212\tval-rmse:2.29153\n",
      "[164]\ttrain-rmse:2.35179\tval-rmse:2.29133\n",
      "[165]\ttrain-rmse:2.35143\tval-rmse:2.29103\n",
      "[166]\ttrain-rmse:2.35102\tval-rmse:2.29058\n",
      "[167]\ttrain-rmse:2.35073\tval-rmse:2.29037\n",
      "[168]\ttrain-rmse:2.35039\tval-rmse:2.29020\n",
      "[169]\ttrain-rmse:2.35022\tval-rmse:2.29005\n",
      "[170]\ttrain-rmse:2.35003\tval-rmse:2.28992\n",
      "[171]\ttrain-rmse:2.34972\tval-rmse:2.28959\n",
      "[172]\ttrain-rmse:2.34949\tval-rmse:2.28942\n",
      "[173]\ttrain-rmse:2.34926\tval-rmse:2.28917\n",
      "[174]\ttrain-rmse:2.34875\tval-rmse:2.28895\n",
      "[175]\ttrain-rmse:2.34851\tval-rmse:2.28881\n",
      "[176]\ttrain-rmse:2.34765\tval-rmse:2.28871\n",
      "[177]\ttrain-rmse:2.34737\tval-rmse:2.28861\n",
      "[178]\ttrain-rmse:2.34683\tval-rmse:2.28801\n",
      "[179]\ttrain-rmse:2.34667\tval-rmse:2.28794\n",
      "[180]\ttrain-rmse:2.34613\tval-rmse:2.28765\n",
      "[181]\ttrain-rmse:2.34581\tval-rmse:2.28751\n",
      "[182]\ttrain-rmse:2.34541\tval-rmse:2.28708\n",
      "[183]\ttrain-rmse:2.34487\tval-rmse:2.28702\n",
      "[184]\ttrain-rmse:2.34462\tval-rmse:2.28680\n",
      "[185]\ttrain-rmse:2.34362\tval-rmse:2.28582\n",
      "[186]\ttrain-rmse:2.34323\tval-rmse:2.28582\n",
      "[187]\ttrain-rmse:2.34273\tval-rmse:2.28532\n",
      "[188]\ttrain-rmse:2.34230\tval-rmse:2.28535\n",
      "[189]\ttrain-rmse:2.34192\tval-rmse:2.28504\n",
      "[190]\ttrain-rmse:2.34173\tval-rmse:2.28487\n",
      "[191]\ttrain-rmse:2.34142\tval-rmse:2.28487\n",
      "[192]\ttrain-rmse:2.34132\tval-rmse:2.28482\n",
      "[193]\ttrain-rmse:2.34109\tval-rmse:2.28456\n",
      "[194]\ttrain-rmse:2.34086\tval-rmse:2.28440\n",
      "[195]\ttrain-rmse:2.34063\tval-rmse:2.28423\n",
      "[196]\ttrain-rmse:2.34041\tval-rmse:2.28397\n",
      "[197]\ttrain-rmse:2.34013\tval-rmse:2.28401\n",
      "[198]\ttrain-rmse:2.33967\tval-rmse:2.28370\n",
      "[199]\ttrain-rmse:2.33957\tval-rmse:2.28361\n",
      "[200]\ttrain-rmse:2.33939\tval-rmse:2.28348\n",
      "[201]\ttrain-rmse:2.33886\tval-rmse:2.28359\n",
      "[202]\ttrain-rmse:2.33868\tval-rmse:2.28350\n",
      "[203]\ttrain-rmse:2.33845\tval-rmse:2.28329\n",
      "[204]\ttrain-rmse:2.33767\tval-rmse:2.28310\n",
      "[205]\ttrain-rmse:2.33747\tval-rmse:2.28298\n",
      "[206]\ttrain-rmse:2.33644\tval-rmse:2.28282\n",
      "[207]\ttrain-rmse:2.33561\tval-rmse:2.28281\n",
      "[208]\ttrain-rmse:2.33542\tval-rmse:2.28259\n",
      "[209]\ttrain-rmse:2.33488\tval-rmse:2.28267\n",
      "[210]\ttrain-rmse:2.33409\tval-rmse:2.28225\n",
      "[211]\ttrain-rmse:2.33389\tval-rmse:2.28247\n",
      "[212]\ttrain-rmse:2.33320\tval-rmse:2.28241\n",
      "[213]\ttrain-rmse:2.33257\tval-rmse:2.28221\n",
      "[214]\ttrain-rmse:2.33211\tval-rmse:2.28182\n",
      "[215]\ttrain-rmse:2.33168\tval-rmse:2.28146\n",
      "[216]\ttrain-rmse:2.33126\tval-rmse:2.28086\n",
      "[217]\ttrain-rmse:2.33030\tval-rmse:2.28087\n",
      "[218]\ttrain-rmse:2.32999\tval-rmse:2.28097\n",
      "[219]\ttrain-rmse:2.32971\tval-rmse:2.28079\n",
      "[220]\ttrain-rmse:2.32949\tval-rmse:2.28067\n",
      "[221]\ttrain-rmse:2.32930\tval-rmse:2.28052\n",
      "[222]\ttrain-rmse:2.32915\tval-rmse:2.28044\n",
      "[223]\ttrain-rmse:2.32871\tval-rmse:2.28029\n",
      "[224]\ttrain-rmse:2.32857\tval-rmse:2.28027\n",
      "[225]\ttrain-rmse:2.32843\tval-rmse:2.28013\n",
      "[226]\ttrain-rmse:2.32803\tval-rmse:2.27993\n",
      "[227]\ttrain-rmse:2.32755\tval-rmse:2.28004\n",
      "[228]\ttrain-rmse:2.32721\tval-rmse:2.28046\n",
      "[229]\ttrain-rmse:2.32710\tval-rmse:2.28038\n",
      "[230]\ttrain-rmse:2.32632\tval-rmse:2.27993\n",
      "[231]\ttrain-rmse:2.32579\tval-rmse:2.27977\n",
      "[232]\ttrain-rmse:2.32571\tval-rmse:2.27971\n",
      "[233]\ttrain-rmse:2.32436\tval-rmse:2.27928\n",
      "[234]\ttrain-rmse:2.32429\tval-rmse:2.27925\n",
      "[235]\ttrain-rmse:2.32425\tval-rmse:2.27920\n",
      "[236]\ttrain-rmse:2.32398\tval-rmse:2.27908\n",
      "[237]\ttrain-rmse:2.32368\tval-rmse:2.27888\n",
      "[238]\ttrain-rmse:2.32253\tval-rmse:2.27892\n",
      "[239]\ttrain-rmse:2.32224\tval-rmse:2.27924\n",
      "[240]\ttrain-rmse:2.32115\tval-rmse:2.27899\n",
      "[241]\ttrain-rmse:2.32086\tval-rmse:2.27898\n",
      "[242]\ttrain-rmse:2.32049\tval-rmse:2.27867\n",
      "[243]\ttrain-rmse:2.32014\tval-rmse:2.27861\n",
      "[244]\ttrain-rmse:2.31982\tval-rmse:2.27915\n",
      "[245]\ttrain-rmse:2.31915\tval-rmse:2.27939\n",
      "[246]\ttrain-rmse:2.31845\tval-rmse:2.27937\n",
      "[247]\ttrain-rmse:2.31834\tval-rmse:2.27927\n",
      "[248]\ttrain-rmse:2.31789\tval-rmse:2.27923\n",
      "[249]\ttrain-rmse:2.31723\tval-rmse:2.27932\n",
      "[250]\ttrain-rmse:2.31691\tval-rmse:2.27916\n",
      "[251]\ttrain-rmse:2.31679\tval-rmse:2.27934\n",
      "[252]\ttrain-rmse:2.31669\tval-rmse:2.27931\n",
      "[253]\ttrain-rmse:2.31590\tval-rmse:2.27876\n",
      "[254]\ttrain-rmse:2.31568\tval-rmse:2.27868\n",
      "[255]\ttrain-rmse:2.31539\tval-rmse:2.27841\n",
      "[256]\ttrain-rmse:2.31527\tval-rmse:2.27848\n",
      "[257]\ttrain-rmse:2.31503\tval-rmse:2.27841\n",
      "[258]\ttrain-rmse:2.31465\tval-rmse:2.27842\n",
      "[259]\ttrain-rmse:2.31445\tval-rmse:2.27873\n",
      "[260]\ttrain-rmse:2.31395\tval-rmse:2.27839\n",
      "[261]\ttrain-rmse:2.31386\tval-rmse:2.27830\n",
      "[262]\ttrain-rmse:2.31331\tval-rmse:2.27823\n",
      "[263]\ttrain-rmse:2.31313\tval-rmse:2.27831\n",
      "[264]\ttrain-rmse:2.31266\tval-rmse:2.27845\n",
      "[265]\ttrain-rmse:2.31250\tval-rmse:2.27831\n",
      "[266]\ttrain-rmse:2.31154\tval-rmse:2.27835\n",
      "[267]\ttrain-rmse:2.31110\tval-rmse:2.27836\n",
      "[268]\ttrain-rmse:2.31002\tval-rmse:2.27816\n",
      "[269]\ttrain-rmse:2.30879\tval-rmse:2.27854\n",
      "[270]\ttrain-rmse:2.30858\tval-rmse:2.27843\n",
      "[271]\ttrain-rmse:2.30825\tval-rmse:2.27820\n",
      "[272]\ttrain-rmse:2.30815\tval-rmse:2.27816\n",
      "[273]\ttrain-rmse:2.30774\tval-rmse:2.27791\n",
      "[274]\ttrain-rmse:2.30745\tval-rmse:2.27781\n",
      "[275]\ttrain-rmse:2.30705\tval-rmse:2.27773\n",
      "[276]\ttrain-rmse:2.30698\tval-rmse:2.27767\n",
      "[277]\ttrain-rmse:2.30674\tval-rmse:2.27764\n",
      "[278]\ttrain-rmse:2.30644\tval-rmse:2.27753\n",
      "[279]\ttrain-rmse:2.30637\tval-rmse:2.27752\n",
      "[280]\ttrain-rmse:2.30618\tval-rmse:2.27747\n",
      "[281]\ttrain-rmse:2.30564\tval-rmse:2.27753\n",
      "[282]\ttrain-rmse:2.30543\tval-rmse:2.27760\n",
      "[283]\ttrain-rmse:2.30517\tval-rmse:2.27755\n",
      "[284]\ttrain-rmse:2.30489\tval-rmse:2.27732\n",
      "[285]\ttrain-rmse:2.30480\tval-rmse:2.27736\n",
      "[286]\ttrain-rmse:2.30453\tval-rmse:2.27708\n",
      "[287]\ttrain-rmse:2.30451\tval-rmse:2.27708\n",
      "[288]\ttrain-rmse:2.30433\tval-rmse:2.27702\n",
      "[289]\ttrain-rmse:2.30395\tval-rmse:2.27699\n",
      "[290]\ttrain-rmse:2.30378\tval-rmse:2.27689\n",
      "[291]\ttrain-rmse:2.30359\tval-rmse:2.27680\n",
      "[292]\ttrain-rmse:2.30324\tval-rmse:2.27680\n",
      "[293]\ttrain-rmse:2.30311\tval-rmse:2.27676\n",
      "[294]\ttrain-rmse:2.30269\tval-rmse:2.27645\n",
      "[295]\ttrain-rmse:2.30230\tval-rmse:2.27648\n",
      "[296]\ttrain-rmse:2.30223\tval-rmse:2.27641\n",
      "[297]\ttrain-rmse:2.30215\tval-rmse:2.27641\n",
      "[298]\ttrain-rmse:2.30185\tval-rmse:2.27644\n",
      "[299]\ttrain-rmse:2.30177\tval-rmse:2.27644\n",
      "[300]\ttrain-rmse:2.30159\tval-rmse:2.27633\n",
      "[301]\ttrain-rmse:2.30134\tval-rmse:2.27609\n",
      "[302]\ttrain-rmse:2.30120\tval-rmse:2.27603\n",
      "[303]\ttrain-rmse:2.30104\tval-rmse:2.27591\n",
      "[304]\ttrain-rmse:2.30091\tval-rmse:2.27589\n",
      "[305]\ttrain-rmse:2.30042\tval-rmse:2.27583\n",
      "[306]\ttrain-rmse:2.30012\tval-rmse:2.27580\n",
      "[307]\ttrain-rmse:2.29982\tval-rmse:2.27576\n",
      "[308]\ttrain-rmse:2.29851\tval-rmse:2.27569\n",
      "[309]\ttrain-rmse:2.29818\tval-rmse:2.27591\n",
      "[310]\ttrain-rmse:2.29794\tval-rmse:2.27590\n",
      "[311]\ttrain-rmse:2.29754\tval-rmse:2.27633\n",
      "[312]\ttrain-rmse:2.29751\tval-rmse:2.27638\n",
      "[313]\ttrain-rmse:2.29744\tval-rmse:2.27641\n",
      "[314]\ttrain-rmse:2.29729\tval-rmse:2.27634\n",
      "[315]\ttrain-rmse:2.29690\tval-rmse:2.27628\n",
      "[316]\ttrain-rmse:2.29629\tval-rmse:2.27604\n",
      "[317]\ttrain-rmse:2.29567\tval-rmse:2.27587\n",
      "[318]\ttrain-rmse:2.29512\tval-rmse:2.27573\n",
      "[319]\ttrain-rmse:2.29497\tval-rmse:2.27575\n",
      "[320]\ttrain-rmse:2.29479\tval-rmse:2.27576\n",
      "[321]\ttrain-rmse:2.29420\tval-rmse:2.27563\n",
      "[322]\ttrain-rmse:2.29396\tval-rmse:2.27562\n",
      "[323]\ttrain-rmse:2.29377\tval-rmse:2.27567\n",
      "[324]\ttrain-rmse:2.29357\tval-rmse:2.27565\n",
      "[325]\ttrain-rmse:2.29345\tval-rmse:2.27580\n",
      "[326]\ttrain-rmse:2.29324\tval-rmse:2.27587\n",
      "[327]\ttrain-rmse:2.29315\tval-rmse:2.27581\n",
      "[328]\ttrain-rmse:2.29307\tval-rmse:2.27583\n",
      "[329]\ttrain-rmse:2.29298\tval-rmse:2.27582\n",
      "[330]\ttrain-rmse:2.29282\tval-rmse:2.27579\n",
      "[331]\ttrain-rmse:2.29270\tval-rmse:2.27576\n",
      "[332]\ttrain-rmse:2.29219\tval-rmse:2.27570\n",
      "[333]\ttrain-rmse:2.29210\tval-rmse:2.27571\n",
      "[334]\ttrain-rmse:2.29196\tval-rmse:2.27570\n",
      "[335]\ttrain-rmse:2.29177\tval-rmse:2.27571\n",
      "[336]\ttrain-rmse:2.29103\tval-rmse:2.27591\n",
      "[337]\ttrain-rmse:2.29044\tval-rmse:2.27576\n",
      "[338]\ttrain-rmse:2.29024\tval-rmse:2.27566\n",
      "[339]\ttrain-rmse:2.29017\tval-rmse:2.27562\n",
      "[340]\ttrain-rmse:2.28991\tval-rmse:2.27605\n",
      "[341]\ttrain-rmse:2.28974\tval-rmse:2.27597\n",
      "[342]\ttrain-rmse:2.28926\tval-rmse:2.27590\n",
      "[343]\ttrain-rmse:2.28905\tval-rmse:2.27576\n",
      "[344]\ttrain-rmse:2.28882\tval-rmse:2.27585\n",
      "[345]\ttrain-rmse:2.28862\tval-rmse:2.27577\n",
      "[346]\ttrain-rmse:2.28850\tval-rmse:2.27573\n",
      "[347]\ttrain-rmse:2.28749\tval-rmse:2.27558\n",
      "[348]\ttrain-rmse:2.28738\tval-rmse:2.27564\n",
      "[349]\ttrain-rmse:2.28703\tval-rmse:2.27553\n",
      "[350]\ttrain-rmse:2.28637\tval-rmse:2.27499\n",
      "[351]\ttrain-rmse:2.28624\tval-rmse:2.27490\n",
      "[352]\ttrain-rmse:2.28613\tval-rmse:2.27490\n",
      "[353]\ttrain-rmse:2.28534\tval-rmse:2.27491\n",
      "[354]\ttrain-rmse:2.28514\tval-rmse:2.27486\n",
      "[355]\ttrain-rmse:2.28467\tval-rmse:2.27454\n",
      "[356]\ttrain-rmse:2.28451\tval-rmse:2.27449\n",
      "[357]\ttrain-rmse:2.28434\tval-rmse:2.27441\n",
      "[358]\ttrain-rmse:2.28411\tval-rmse:2.27436\n",
      "[359]\ttrain-rmse:2.28399\tval-rmse:2.27432\n",
      "[360]\ttrain-rmse:2.28383\tval-rmse:2.27429\n",
      "[361]\ttrain-rmse:2.28352\tval-rmse:2.27413\n",
      "[362]\ttrain-rmse:2.28327\tval-rmse:2.27406\n",
      "[363]\ttrain-rmse:2.28314\tval-rmse:2.27400\n",
      "[364]\ttrain-rmse:2.28301\tval-rmse:2.27397\n",
      "[365]\ttrain-rmse:2.28271\tval-rmse:2.27379\n",
      "[366]\ttrain-rmse:2.28204\tval-rmse:2.27325\n",
      "[367]\ttrain-rmse:2.28184\tval-rmse:2.27327\n",
      "[368]\ttrain-rmse:2.28079\tval-rmse:2.27308\n",
      "[369]\ttrain-rmse:2.28056\tval-rmse:2.27303\n",
      "[370]\ttrain-rmse:2.28011\tval-rmse:2.27288\n",
      "[371]\ttrain-rmse:2.27993\tval-rmse:2.27302\n",
      "[372]\ttrain-rmse:2.27976\tval-rmse:2.27294\n",
      "[373]\ttrain-rmse:2.27960\tval-rmse:2.27286\n",
      "[374]\ttrain-rmse:2.27949\tval-rmse:2.27278\n",
      "[375]\ttrain-rmse:2.27938\tval-rmse:2.27274\n",
      "[376]\ttrain-rmse:2.27919\tval-rmse:2.27277\n",
      "[377]\ttrain-rmse:2.27909\tval-rmse:2.27273\n",
      "[378]\ttrain-rmse:2.27883\tval-rmse:2.27271\n",
      "[379]\ttrain-rmse:2.27874\tval-rmse:2.27282\n",
      "[380]\ttrain-rmse:2.27785\tval-rmse:2.27284\n",
      "[381]\ttrain-rmse:2.27778\tval-rmse:2.27287\n",
      "[382]\ttrain-rmse:2.27757\tval-rmse:2.27286\n",
      "[383]\ttrain-rmse:2.27680\tval-rmse:2.27299\n",
      "[384]\ttrain-rmse:2.27635\tval-rmse:2.27321\n",
      "[385]\ttrain-rmse:2.27614\tval-rmse:2.27316\n",
      "[386]\ttrain-rmse:2.27584\tval-rmse:2.27305\n",
      "[387]\ttrain-rmse:2.27540\tval-rmse:2.27262\n",
      "[388]\ttrain-rmse:2.27508\tval-rmse:2.27301\n",
      "[389]\ttrain-rmse:2.27499\tval-rmse:2.27305\n",
      "[390]\ttrain-rmse:2.27437\tval-rmse:2.27305\n",
      "[391]\ttrain-rmse:2.27399\tval-rmse:2.27308\n",
      "[392]\ttrain-rmse:2.27376\tval-rmse:2.27307\n",
      "[393]\ttrain-rmse:2.27265\tval-rmse:2.27281\n",
      "[394]\ttrain-rmse:2.27255\tval-rmse:2.27281\n",
      "[395]\ttrain-rmse:2.27180\tval-rmse:2.27269\n",
      "[396]\ttrain-rmse:2.27148\tval-rmse:2.27273\n",
      "[397]\ttrain-rmse:2.27133\tval-rmse:2.27273\n",
      "[398]\ttrain-rmse:2.27108\tval-rmse:2.27286\n",
      "[399]\ttrain-rmse:2.27094\tval-rmse:2.27287\n",
      "[400]\ttrain-rmse:2.27079\tval-rmse:2.27284\n",
      "[401]\ttrain-rmse:2.26996\tval-rmse:2.27300\n",
      "[402]\ttrain-rmse:2.26993\tval-rmse:2.27296\n",
      "[403]\ttrain-rmse:2.26977\tval-rmse:2.27310\n",
      "[404]\ttrain-rmse:2.26965\tval-rmse:2.27333\n",
      "[405]\ttrain-rmse:2.26948\tval-rmse:2.27334\n",
      "[406]\ttrain-rmse:2.26932\tval-rmse:2.27351\n",
      "[407]\ttrain-rmse:2.26918\tval-rmse:2.27345\n",
      "Stopping. Best iteration:\n",
      "[387]\ttrain-rmse:2.27540\tval-rmse:2.27262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# eta = 0.075 (slow xgb training)\n",
    "validation, xgb_tg_encode = run_xgb(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the xgb model\n",
    "import pickle\n",
    "pickle.dump(xgb_tg_encode, open(\"xgb_tg_encode.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved xgb model \n",
    "xgb_tg_encode = pickle.load(open(\"xgb_tg_encode.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = data[(data['date'] > 20160424) & (data['date'] <= 20160522)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data[data['date'] > 20160522]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_tg_encode.predict(xgb.DMatrix(test, test['demand']))\n",
    "test['pred_demand'] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function `get_sub` to save the correct format for submissions file where we predict 28 forecast days (F1-F28) of items sold for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub(validation, test, submission):\n",
    "    val_true = validation[['demand']]\n",
    "    val_true = val_true.merge(id_date, left_index=True, right_index=True)\n",
    "    val_true = pd.pivot(val_true, index = 'id', columns = 'date', values = 'demand').reset_index()\n",
    "    \n",
    "    val_true.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "    val_true['id'] = val_true['id'].str.replace('_evaluation','_validation')\n",
    "    \n",
    "    eval_pred = test[['pred_demand']]\n",
    "    eval_pred = eval_pred.merge(id_date, left_index=True, right_index=True)\n",
    "    eval_pred = pd.pivot(eval_pred, index = 'id', columns = 'date', values = 'pred_demand').reset_index()\n",
    "    \n",
    "    # change column name in consistent with sample_submission, like convert 20160523 to F1\n",
    "    eval_pred.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "    \n",
    "    final = pd.concat([val_true, eval_pred])\n",
    "    \n",
    "    # make sure the id order is the same as sample_submission\n",
    "    final = submission[['id']].merge(final, on = 'id') \n",
    "    final.to_csv('xgb_submission.csv', index = False)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = get_sub(validation, test, submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
